import streamlit as st

import pandas as pd
import seaborn as sns


import matplotlib.pyplot as plt
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from keras.models import Sequential


# åŠ è½½æ•°æ®
df = pd.read_excel("Global.xlsx")
@st.cache_data
def load_data():
    return pd.read_excel("Global.xlsx")

def plot_average_earnings_by_category(df):
    # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„æ”¶å…¥å¹³å‡å€¼
    avg_lowest_earnings = df.groupby('Category')['Lowest Monthly Earnings'].mean().reset_index()
    avg_highest_earnings = df.groupby('Category')['Highest Monthly Earnings'].mean().reset_index()

    # ç»˜åˆ¶æŸ±çŠ¶å›¾æ¯”è¾ƒå¹³å‡æ”¶å…¥
    fig_lowest, ax_lowest = plt.subplots(figsize=(12, 6))
    sns.barplot(x='Category', y='Lowest Monthly Earnings', data=avg_lowest_earnings, ax=ax_lowest)
    ax_lowest.set_title('Average Lowest Monthly Earnings by Category')
    ax_lowest.set_xticklabels(avg_lowest_earnings['Category'], rotation=45)
    st.pyplot(fig_lowest)


# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Global Youtube Statistics",
    page_icon="ğŸ‚",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åŠ è½½æ•°æ®
df = load_data()

# ä¾§è¾¹æ æ ‡é¢˜
st.sidebar.title('Global Youtube Statistics')

# åœ¨ä¾§è¾¹æ ä¸­é€‰æ‹©å…¶ä»–æ¡ä»¶ï¼ˆä¾‹å¦‚åˆ†ç±»ï¼‰
category_list = list(df.Category.unique())
selected_category = st.sidebar.selectbox('Select a category', category_list)

# æ ¹æ®é€‰æ‹©çš„åˆ†ç±»ç­›é€‰æ•°æ®
df_filtered_by_category = df[df.Category == selected_category]

# æ˜¾ç¤ºè¯äº‘å›¾
def generate_wordcloud(df):
    youtuber_counts = df['Youtuber'].value_counts().to_dict()
    wc = WordCloud(width=800, height=500, background_color='white').generate_from_frequencies(youtuber_counts)
    return wc

# æ˜¾ç¤ºè¯äº‘å›¾
if st.sidebar.button('Show By Category'):
    #è¯äº‘å›¾
    st.write(f"Word Cloud for Category '{selected_category}'")
    wc_category = generate_wordcloud(df_filtered_by_category)
    st.image(wc_category.to_array(), use_column_width=True)

    #çƒ­åŠ›å›¾
    st.write(f"Heatmap for Category '{selected_category}'")
    heatmap_data = df_filtered_by_category.corr()
    sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", fmt=".2f")
    st.pyplot(plt.gcf())
    st.title(f'Income Analysis for YouTubers in Category: {selected_category}')

    # è·å–é€‰å®šç±»åˆ«çš„æ•°æ®
    category_data = df[df['Category'] == selected_category]

    # ç»˜åˆ¶æ”¶å…¥åˆ†å¸ƒå›¾
    st.subheader('Income Distribution')
    fig, axes = plt.subplots(2, 1, figsize=(12, 12))
    sns.histplot(category_data['Lowest Monthly Earnings'], bins=10, kde=True, ax=axes[0])
    axes[0].set_title('Lowest Monthly Earnings Distribution')
    sns.histplot(category_data['Highest Monthly Earnings'], bins=10, kde=True, ax=axes[1])
    axes[1].set_title('Highest Monthly Earnings Distribution')
    plt.tight_layout()
    st.pyplot(fig)

def plot_comparison_of_earnings(df):
    # è®¡ç®—æ‰€æœ‰ç±»åˆ«çš„æ”¶å…¥å¹³å‡å€¼
    avg_lowest_earnings = df.groupby('Category')['Lowest Monthly Earnings'].mean().reset_index()
    avg_highest_earnings = df.groupby('Category')['Highest Monthly Earnings'].mean().reset_index()

    # ç»˜åˆ¶æŸ±çŠ¶å›¾æ¯”è¾ƒå¹³å‡æ”¶å…¥
    fig, axes = plt.subplots(1, 2, figsize=(18, 6))
    
    sns.barplot(x='Category', y='Lowest Monthly Earnings', data=avg_lowest_earnings, ax=axes[0])
    axes[0].set_title('Average Lowest Monthly Earnings by Category')
    axes[0].set_xticklabels(avg_lowest_earnings['Category'], rotation=45)

    sns.barplot(x='Category', y='Highest Monthly Earnings', data=avg_highest_earnings, ax=axes[1])
    axes[1].set_title('Average Highest Monthly Earnings by Category')
    axes[1].set_xticklabels(avg_highest_earnings['Category'], rotation=45)

    plt.tight_layout()
    st.pyplot(fig)

# è®¡ç®—å¹¶ç»˜åˆ¶æ‰€æœ‰ç±»åˆ«ä¹‹é—´çš„ 'Lowest Monthly Earnings' å’Œ 'Highest Monthly Earnings' çš„æ¯”è¾ƒ
if st.sidebar.button('Compare Earnings'):
    plot_comparison_of_earnings(df)

interested_features = ['Youtuber', 'Subscribers', 'Video Views', 'Uploads', 'Category',
       'Country', 'Abbreviation','Gross Tertiary Education Enrollment (%)',
       'Unemployment Rate', 'Population', 'Urban Population', 'Year','Lowest Monthly Earnings','Highest Monthly Earnings']

# è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
correlation_matrix = df[interested_features].corr(numeric_only=True)


# æ‰¾åˆ°é™¤äº†'Lowest Monthly Earnings'å’Œ'Highest Monthly Earnings'ä¹‹å¤–çš„æœ€ç›¸å…³çš„ç‰¹å¾
most_correlated_features = correlation_matrix[
    ['Lowest Monthly Earnings', 'Highest Monthly Earnings']
].abs().sort_values(by=['Lowest Monthly Earnings', 'Highest Monthly Earnings'], ascending=False).head(5)

# åœ¨ Streamlit ä¸­å±•ç¤ºç»“æœ
st.write('### Top 5 Most Correlated Features with Lowest and Highest Monthly Earnings:')
st.write(most_correlated_features)

correlation_matrix = df[interested_features].corr()

# æ‰¾åˆ°é™¤äº†'Lowest Monthly Earnings'å’Œ'Highest Monthly Earnings'ä¹‹å¤–çš„æœ€ç›¸å…³çš„ç‰¹å¾
correlation_matrix = correlation_matrix.drop(['Lowest Monthly Earnings', 'Highest Monthly Earnings'], errors='ignore')
most_correlated_features = correlation_matrix[
    ['Lowest Monthly Earnings', 'Highest Monthly Earnings']
].abs().sort_values(by=['Lowest Monthly Earnings', 'Highest Monthly Earnings','Lowest Monthly Earnings', 'Highest Monthly Earnings'], ascending=False).head(5)

# åœ¨ Streamlit ä¸­å±•ç¤ºç»“æœ
st.write('### Top 5 Most Correlated Features with Lowest and Highest Monthly Earnings:')
st.write(most_correlated_features)
print(most_correlated_features)

# å¯è§†åŒ–ç›¸å…³æ€§çŸ©é˜µ
plt.figure(figsize=(10, 8))
sns.heatmap(most_correlated_features, annot=True, cmap='coolwarm')
st.pyplot(plt)

# é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡
features = ['Video Views', 'Subscribers', 'Uploads', 'Population', 'Urban Population']
target = ['Lowest Monthly Earnings', 'Highest Monthly Earnings']

X = df[features]
y = df[target]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# å®šä¹‰æ¨¡å‹
model = Sequential()
model.add(Dense(10, input_dim=len(features), activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(2))  # ä¸¤ä¸ªè¾“å‡ºç¥ç»å…ƒï¼Œæ¯ä¸ªç›®æ ‡å˜é‡ä¸€ä¸ª

# ç¼–è¯‘æ¨¡å‹
model.compile(loss='mean_squared_error', optimizer='adam')

# è®­ç»ƒæ¨¡å‹
model.fit(X_train, y_train, epochs=50, batch_size=10)

# è¯„ä¼°æ¨¡å‹
loss = model.evaluate(X_test, y_test)
st.write(f'Test loss: {loss}')

# è¿›è¡Œé¢„æµ‹
predictions = model.predict(X_test)
st.write('Predictions:', predictions)

